{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lks7fVEGASS-",
        "outputId": "7efc218b-113b-4399-a1c5-d048c85b3070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langgraph==0.2.45 langchain-google-genai==2.0.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "api_key = \"AIzaSyAfNLUxZF1saOTHeUPdNnmbDJmnnjcKBwY\"\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "# Configure the generative AI client with the API key\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
      ],
      "metadata": {
        "id": "4V3CPg6sAUlg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class OrderState(TypedDict):\n",
        "    \"\"\"State representing the customer's order conversation.\"\"\"\n",
        "\n",
        "    # The chat conversation. This preserves the conversation history between nodes. The `add_messages` annotation indicates to LangGraph that state is updated by appending returned messages, not replacing them.\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "    # The customer's in-progress order.\n",
        "    order: list[str]\n",
        "\n",
        "    # Flag indicating if the customer has completed their order.\n",
        "    finished: bool\n",
        "\n",
        "\n",
        "# The system instruction defines how the chatbot is expected to behave and includes rules for when to call different functions, as well as rules for the conversation, such as tone and what is permitted for discussion.\n",
        "ORDERBOT_SYSINT = (\n",
        "    \"system\", # the system indicates the message is a system instruction\n",
        "\n",
        "    \"You are an OrderBot, an interactive cafe ordering system. A human will talk to you about the\"\n",
        "    \"available \"\n",
        ")\n",
        "\n",
        "# This is the message with which the system opens the conversation.\n",
        "WELCOME_MSG = \"Welcome to the PESU cafe! Type `q` to quit at any time. How may I serve you today?\""
      ],
      "metadata": {
        "id": "Gfm2u_qtwwHv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "def chatbot(state: OrderState) -> OrderState:\n",
        "    \"\"\"The chatbot itself. A simple wrapper around the model's own chat interface.\"\"\"\n",
        "    message_history = [ORDERBOT_SYSINT] +  state[\"messages\"]\n",
        "    return {\"messages\": [llm.invoke(message_history)]}\n",
        "\n",
        "# Set up the initial graph based on our state definition.\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Add the chatbot function to the app graph as a node called \"chatbot\".\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "# Define the chatbot node as the app entry point.\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "chat_graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "ClCeXqMIwyoM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "Image(chat_graph.get_graph().draw_mermaid_png())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "KXUIIenlw1Ns",
        "outputId": "6baa53bb-1fc0-4db8-a930-1ccfb81abe66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "user_msg = \"Hello, what can you do?\"\n",
        "state = chat_graph.invoke({\"messages\": [user_msg]})\n",
        "\n",
        "# The state object contains lots of information. Unconnect the pprint lines to see it all.\n",
        "# pprint(state)\n",
        "\n",
        "# Note that the final state now has 2 messages. Our HumanMessage, and an additonal AIMessage.\n",
        "for msg in state[\"messages\"]:\n",
        "    print(f\"{type(msg).__name__}:{msg.content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1IpQ9olw3VL",
        "outputId": "915e1b99-0a6c-474e-8b14-1cd2359fe02f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HumanMessage:Hello, what can you do?\n",
            "AIMessage:Hello! I'm OrderBot, your friendly cafe ordering assistant.  I can help you browse our menu, place your order, and provide information about our food and drinks.  What would you like to do?  Do you want to see the menu?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_msg = \"Oh great, what kinds of latte can you make?\"\n",
        "\n",
        "state[\"messages\"].append(user_msg)\n",
        "state = chat_graph.invoke(state)\n",
        "\n",
        "# pprint(state)\n",
        "for msg in state[\"messages\"]:\n",
        "  print(f\"{type(msg).__name__}:{msg.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYrAbf2Fw42M",
        "outputId": "ffd32dfa-f126-4574-8af5-6f559f2efb9c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HumanMessage:Hello, what can you do?\n",
            "AIMessage:Hello! I'm OrderBot, your friendly cafe ordering assistant.  I can help you browse our menu, place your order, and provide information about our food and drinks.  What would you like to do?  Do you want to see the menu?\n",
            "\n",
            "HumanMessage:Oh great, what kinds of latte can you make?\n",
            "AIMessage:We offer a variety of delicious lattes!  Currently, we have:\n",
            "\n",
            "* **Classic Latte:** Our signature espresso-based latte with steamed milk and a thin layer of foam.\n",
            "* **Vanilla Latte:** A classic latte with the addition of vanilla syrup.\n",
            "* **Caramel Latte:** A classic latte with a rich caramel drizzle and syrup.\n",
            "* **Mocha Latte:** A classic latte combined with chocolate syrup.\n",
            "* **Hazelnut Latte:** A classic latte with hazelnut syrup.\n",
            "* **Pumpkin Spice Latte (Seasonal):**  A warm and comforting latte with pumpkin spice flavoring (available during fall).\n",
            "\n",
            "\n",
            "Would you like to hear more about any of these, or perhaps see our other coffee options?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages.ai import AIMessage\n",
        "\n",
        "def human_node(state: OrderState) -> OrderState:\n",
        "  \"\"\"Display the last model message to the user, and receive the user's input.\"\"\"\n",
        "  last_msg = state[\"messages\"][-1]\n",
        "  print(\"Model:\", last_msg.content)\n",
        "\n",
        "  user_input = input(\"User: \")\n",
        "\n",
        "  # if it looks like the user is trying to quit, flag the conversation as over.\n",
        "  if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n",
        "    state[\"finished\"] = True\n",
        "\n",
        "  return state | {\"messages\": [(\"user\", user_input)]}\n",
        "\n",
        "\n",
        "def chatbot_with_welcome_msg(state: OrderState) -> OrderState:\n",
        "  \"\"\"The chatbot itself. A wrapper around the model's own chat interface.\"\"\"\n",
        "\n",
        "  if state[\"messages\"]:\n",
        "    # If there are messages, continue the conversation with the Gemini model.\n",
        "    new_output = llm.invoke([ORDERBOT_SYSINT] + state[\"messages\"])\n",
        "  else:\n",
        "    # If there are no messages, start with the welcome message.\n",
        "    new_output = AIMessage(content=WELCOME_MSG)\n",
        "\n",
        "  return state | {\"messages\": [new_output]}\n",
        "\n",
        "# Start building a new graph.\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Add the chatbot and human nodes to the app graph.\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_welcome_msg)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "\n",
        "# Start with the chatbot again.\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "# The chatbot will always go to the human next.\n",
        "graph_builder.add_edge(\"chatbot\", \"human\");\n"
      ],
      "metadata": {
        "id": "PrsrdY5dw6z-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "def maybe_exit_human_node(state: OrderState) -> Literal[\"chatbot\", \"__end__\"]:\n",
        "  \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n",
        "  if state.get(\"finished\", False):\n",
        "    return END\n",
        "  else:\n",
        "    return \"chatbot\"\n",
        "\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
        "\n",
        "chat_with_human_graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "XPRtxdSBw9mK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remember that this will loop forever, unless you input `q`, `quit` or one of the other exit terms defined in `human_node`.\n",
        "\n",
        "state = chat_with_human_graph.invoke({\"messages\": []})\n",
        "\n",
        "# - 'q' to exit.\n",
        "\n",
        "# pprint(state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yhZMNJNw_NY",
        "outputId": "ee5347e3-2506-4cb2-a50a-2f0725420fbe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Welcome to the PESU cafe! Type `q` to quit at any time. How may I serve you today?\n",
            "User: hi\n",
            "Model: Hello!  What can I get for you?  We have coffee, tea, pastries, and sandwiches.  Would you like to see a menu?\n",
            "\n",
            "User: yes\n",
            "Model: Okay, here's our menu:\n",
            "\n",
            "**Coffee:**\n",
            "\n",
            "* Espresso - ₹80\n",
            "* Americano - ₹90\n",
            "* Latte - ₹100\n",
            "* Cappuccino - ₹110\n",
            "* Mocha - ₹120\n",
            "\n",
            "**Tea:**\n",
            "\n",
            "* Black Tea - ₹50\n",
            "* Green Tea - ₹60\n",
            "* Masala Chai - ₹70\n",
            "\n",
            "**Pastries:**\n",
            "\n",
            "* Croissant - ₹60\n",
            "* Muffin - ₹50\n",
            "* Brownie - ₹70\n",
            "\n",
            "**Sandwiches:**\n",
            "\n",
            "* Veggie Sandwich - ₹80\n",
            "* Chicken Sandwich - ₹100\n",
            "\n",
            "\n",
            "What would you like to order?\n",
            "\n",
            "User: croissant\n",
            "Model: Okay, one croissant.  Anything else for you today?\n",
            "\n",
            "User: no\n",
            "Model: Okay, that's one croissant. Your total is ₹60.  Is there anything else I can help you with?\n",
            "\n",
            "User: no\n",
            "Model: Great! Your order of one croissant (₹60) is on its way.  Enjoy!  Is there anything else I can assist you with?\n",
            "\n",
            "User: no\n",
            "Model: Enjoy your croissant!  Have a great day!\n",
            "\n",
            "User: q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def get_menu() -> str:\n",
        "  \"\"\"Provide the latest up-to-date menu.\"\"\"\n",
        "  # Note that this is just hard-coded text, but you could connect this to a live stock database, or you could use Gemini's multi-modal capabilities and take live photos of your cafe's chalk menu or the products on the counter and assemble them into an input.\n",
        "\n",
        "  return \"\"\"\n",
        "  MENU:\n",
        "\n",
        "  Coffee Drinks\n",
        "    Espresso\n",
        "    Americano\n",
        "    Cold Brew\n",
        "    Filter Coffee (South Indian Style)\n",
        "    Instant Coffee\n",
        "\n",
        "  Coffee Drink with Milk\n",
        "    Latte\n",
        "    Cappuccino\n",
        "    Mocha\n",
        "    Flat White\n",
        "    Macchiato\n",
        "    Cotado\n",
        "\n",
        "  Tea Drinks\n",
        "    Masala Chai\n",
        "    Ginger Tea (Adrak Chai)\n",
        "    Cardamom Tea (Elachi Tea)\n",
        "    Lemon Tea\n",
        "    Green Tea\n",
        "    Tulsi Tea\n",
        "\n",
        "  Traditional Indian Beverages\n",
        "    Filter Kaapi (Filter Coffee)\n",
        "    Buttermilk (Chaas)\n",
        "    Cold Coffee\n",
        "    Rose Milk\n",
        "    Lassi (Sweet, Salted, Mango)\n",
        "\n",
        "  Other Drinks\n",
        "    Steamer\n",
        "    Hot Chocolate\n",
        "    Falooda\n",
        "    Tender Coconut Water\n",
        "\n",
        "  Food Items\n",
        "    Sanwiches (Veg, Chicken, Tuna)\n",
        "    Wraps (Falafel, Grilled Veg)\n",
        "    Salads (Caesar, Garden, Greek)\n",
        "    Pastries (Croissant, Muffin, Danish)\n",
        "    Cakes (CHeesecake, Chocolatte, Red Velvet)\n",
        "    Snacks (Samosa, Pakora, French Fries)\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "p0h_oKPbxArw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# Define the tools and create a \"tools\" node.\n",
        "tools = [get_menu]\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "# Attach the tools to the model so that it knows what it can call.\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def maybe_route_to_tools(state: OrderState) -> Literal[\"tools\", \"human\"]:\n",
        "  \"\"\"Route between human or tool nodes, depending if a tool call is made.\"\"\"\n",
        "  if not (msgs := state.get(\"messages\", [])):\n",
        "    raise ValueError(f\"No messages found when parsing state: {state}\")\n",
        "\n",
        "  # Only route based on the last message.\n",
        "  msg = msgs[-1]\n",
        "\n",
        "  # When the chatbot returns tool_calls, route to the \"tools\" node.\n",
        "  if hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n",
        "    return \"tools\"\n",
        "  else:\n",
        "    return \"human\"\n",
        "\n",
        "def chatbot_with_tools(state: OrderState) -> OrderState:\n",
        "  \"\"\"The chatbot with tools. A simple wrapper around the model's own chat interface.\"\"\"\n",
        "  defaults = {\"order\": [], \"finished\": False}\n",
        "\n",
        "  if state[\"messages\"]:\n",
        "    new_output = llm_with_tools.invoke([ORDERBOT_SYSINT] + state[\"messages\"])\n",
        "  else:\n",
        "    new_output = AIMessage(content=WELCOME_MSG)\n",
        "\n",
        "  # Set up some defaults if not already set, then pass through the provided state, overriding only the \"messages\" field.\n",
        "  return defaults | state | {\"messages\": [new_output]}\n",
        "\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Add the nodes, including the new tool_node.\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Chatbot may go to tools, or human.\n",
        "graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n",
        "# Human may go back to chatbot, or exit.\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
        "\n",
        "# Tools always route back to chat afterwards.\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_with_menu = graph_builder.compile()"
      ],
      "metadata": {
        "id": "Vyo9OopyxNxh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remember that you have not implemented ordering yet, so this will loop forever, unless you input `q`, `quit` or one of the other exit terms defined in the `human_node`.\n",
        "# Uncomment this line to execute the graph:\n",
        "state = graph_with_menu.invoke({\"messages\": []})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UlDCTMzxPBW",
        "outputId": "561a655b-46f4-49be-84a5-52c40b91326f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Welcome to the PESU cafe! Type `q` to quit at any time. How may I serve you today?\n",
            "User: hi\n",
            "Model: Hello there!  May I take your order?\n",
            "\n",
            "User: yes\n",
            "Model: Here's our menu:\n",
            "\n",
            "MENU:\n",
            "\n",
            "Coffee Drinks\n",
            "  Espresso\n",
            "  Americano\n",
            "  Cold Brew\n",
            "  Filter Coffee (South Indian Style)\n",
            "  Instant Coffee\n",
            "\n",
            "Coffee Drink with Milk\n",
            "  Latte\n",
            "  Cappuccino\n",
            "  Mocha\n",
            "  Flat White\n",
            "  Macchiato\n",
            "  Cotado\n",
            "\n",
            "Tea Drinks\n",
            "  Masala Chai\n",
            "  Ginger Tea (Adrak Chai)\n",
            "  Cardamom Tea (Elachi Tea)\n",
            "  Lemon Tea\n",
            "  Green Tea\n",
            "  Tulsi Tea\n",
            "\n",
            "Traditional Indian Beverages\n",
            "  Filter Kaapi (Filter Coffee)\n",
            "  Buttermilk (Chaas)\n",
            "  Cold Coffee\n",
            "  Rose Milk\n",
            "  Lassi (Sweet, Salted, Mango)\n",
            "\n",
            "Other Drinks\n",
            "  Steamer\n",
            "  Hot Chocolate\n",
            "  Falooda\n",
            "  Tender Coconut Water\n",
            "\n",
            "Food Items\n",
            "  Sanwiches (Veg, Chicken, Tuna)\n",
            "  Wraps (Falafel, Grilled Veg)\n",
            "  Salads (Caesar, Garden, Greek)\n",
            "  Pastries (Croissant, Muffin, Danish)\n",
            "  Cakes (CHeesecake, Chocolatte, Red Velvet)\n",
            "  Snacks (Samosa, Pakora, French Fries)\n",
            "\n",
            "\n",
            "What would you like to order?\n",
            "\n",
            "User: q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Iterable\n",
        "from random import randint\n",
        "\n",
        "from langgraph.prebuilt import InjectedState\n",
        "from langchain_core.messages.tool import ToolMessage\n",
        "\n",
        "# These functions have no body; LangGraph does not allow @tools to update the conversation state, so you will implement a separate node to handle state updates. Using @tools is still very convenient for defining the tool schema, so empty functions have been defined that will bound to the LLM but their implementation is deferred to the order_node.\n",
        "@tool\n",
        "def add_to_order(item: str) -> str:\n",
        "  \"\"\"Adds the specified time to the customer's order.\n",
        "\n",
        "  Returns:\n",
        "    The updated order in progress.\n",
        "  \"\"\"\n",
        "\n",
        "@tool\n",
        "def confirm_oder() -> str:\n",
        "  \"\"\"Ask the customer if the order is correct.\n",
        "\n",
        "  Returns:\n",
        "    The user's free-text response.\n",
        "  \"\"\"\n",
        "\n",
        "@tool\n",
        "def get_order() -> str:\n",
        "  \"\"\"Returns the user's order so far. One item per line.\"\"\"\n",
        "\n",
        "@tool\n",
        "def clear_order() -> str:\n",
        "  \"\"\"Removes all items from the user's order.\"\"\"\n",
        "\n",
        "@tool\n",
        "def place_order() -> str:\n",
        "  \"\"\"Sends the order to the barista for fulfillment.\n",
        "\n",
        "  Returns:\n",
        "    The estimated number of minutes until the order is ready.\n",
        "  \"\"\"\n",
        "\n",
        "@tool\n",
        "def order_node(state: OrderState) -> OrderState:\n",
        "  \"\"\" Ther ordering node. This is where the order state is manipulated.\"\"\"\n",
        "  tool_msg = state.get(\"messages\", [])[-1]\n",
        "  order = state.get(\"order\", [])\n",
        "  outbound_msgs = []\n",
        "  order_placed = False\n",
        "\n",
        "  for tool_call in tool_msg.tool_calls:\n",
        "    if tool_call[\"name\"] == \"add_to_order\":\n",
        "      order.append(f'{tool_call[\"args\"][\"item\"]}')\n",
        "      response = \"\\n\".join(order)\n",
        "\n",
        "    elif tool_call[\"name\"] == \"confirm_order\":\n",
        "\n",
        "      # We could entrust the LLM to do order confimation but it is a good practice to show the user the exact data that comprises their order so that what they confirm precisely matches the order that goes to the kitchen = avoiding hallucination or reality skew.\n",
        "\n",
        "      # In a real scenario, this is where you would connect your POS screen to show the order to the user\n",
        "\n",
        "      print(\"Your Order:\")\n",
        "      if not order:\n",
        "        print(\" (No items)\")\n",
        "      for drink in order:\n",
        "        print(f\" {drink}\")\n",
        "\n",
        "      response = input(\"Is this correct? \")\n",
        "\n",
        "    elif tool_call[\"name\"] == \"get_order\":\n",
        "      response = \"\\n\".join(order) if order else \"(no order)\"\n",
        "\n",
        "    elif tool_call[\"name\"] == \"clear_order\":\n",
        "      order.clear()\n",
        "      response = None\n",
        "\n",
        "    elif tool_call[\"name\"] == \"place_order\":\n",
        "      order_text = \"\\n\".join(order)\n",
        "      print(\"Sending order to kitchen!\")\n",
        "      print(order_text)\n",
        "\n",
        "      # TODO(you!): Implement cafe.\n",
        "      order_placed = True\n",
        "      response = randint(1,5) # ETA in minutes\n",
        "\n",
        "    else:\n",
        "      raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n",
        "\n",
        "    # Record the tool results as tool messages.\n",
        "    outbound_msgs.append(\n",
        "      ToolMessage(\n",
        "        content = response,\n",
        "        name = tool_call[\"name\"],\n",
        "        tool_call_id = tool_call[\"id\"]\n",
        "      )\n",
        "    )\n",
        "  return {\"messages\": outbound_msgs, \"order\": order, \"finished\":order_placed}\n",
        "\n",
        "def maybe_route_to_tools(state:OrderState) -> str:\n",
        "  \"\"\"Route between chat and tool nodes if a tool call is made.\"\"\"\n",
        "  if not (msgs := state.get(\"messages\", [])):\n",
        "    raise ValueError(f\"No messages found when parsing state: {state}\")\n",
        "\n",
        "  msg = msgs[-1]\n",
        "\n",
        "  if state.get(\"finished\", False):\n",
        "    # When an order is placed, exit the app. The system instruction indicates that the chatbot should say thanks and goodbye at this point, so we can exit cleanly.\n",
        "    return END\n",
        "\n",
        "  elif hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n",
        "    # Route to `tools` node for any automated tool calls first.\n",
        "    if any(\n",
        "        tool[\"name\"] in tool_node.tools_by_name.keys() for tool in msg.tool_calls\n",
        "    ):\n",
        "      return \"tools\"\n",
        "    else:\n",
        "      return \"ordering\"\n",
        "\n",
        "  else:\n",
        "    return \"human\""
      ],
      "metadata": {
        "id": "S4J1CXq2xQT-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto-tools will be invoked automatically by the ToolNode\n",
        "auto_tools = [get_menu]\n",
        "tool_node = ToolNode(auto_tools)\n",
        "\n",
        "# Order-tools will be handled by the order node.\n",
        "order_tools = [add_to_order, confirm_oder, get_order, clear_order, place_order]\n",
        "\n",
        "# the LLM needs to know about all of the tools, so specify everything here.\n",
        "llm_with_tools = llm.bind_tools(auto_tools + order_tools)\n",
        "\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Nodes\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "graph_builder.add_node(\"ordering\", order_node)\n",
        "\n",
        "# Chatbot -> {ordering, tools, human, END}\n",
        "graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n",
        "# Human -> {chatbot, END}\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
        "\n",
        "# Tools (both kinds) always route back to the chat afterwards.\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(\"ordering\", \"chatbot\")\n",
        "\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_with_order_tools = graph_builder.compile()"
      ],
      "metadata": {
        "id": "pcxI6YCtxVrQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The default recursion limit for traversing nodes is 25 - setting is higher.\n",
        "# means you can try a more complex order with multiple steps and round-trips.\n",
        "config = {\"recursion_limit\" : 100}\n",
        "\n",
        "# Uncomment this line to execute the graph:\n",
        "state = graph_with_order_tools.invoke({\"messages\": []}, config)\n",
        "\n",
        "# Things to try:\n",
        "# - Order a drink!\n",
        "# - Make a change to your order.\n",
        "# - \"Which teas are from England?\"\n",
        "# - Note that the graph should naturally exit after placing an order.\n",
        "\n",
        "# pprint(state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUhmM4mFxXFV",
        "outputId": "d0839a71-2f6b-446e-d3cf-1d17f30be5d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Welcome to the PESU cafe! Type `q` to quit at any time. How may I serve you today?\n",
            "User: q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-A-I93rFxY-h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}